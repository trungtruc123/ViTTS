# <img src="https://github.com/trungtruc123/ViTTS/blob/develop/images/logo.png"/>

ğŸViTTS is a library for advanced Text-to-Speech generation for multi language such as: chinese, japanese, vietnamese .

ğŸViTTS builts on the latest research, It designed to achieve the best trade-off among ease of training, inference and evaluate.

ğŸViTTS is a library for text to speech, it achive performance in speech and quality.


## ğŸ’¬ Where to ask questions
Please use our dedicated channels for questions and discussion. Help is much more valuable if it's shared publicly so that more people can benefit from it.

| Type                            | Platforms      |
| ------------------------------- |----------------|
| ğŸš¨ **Bug Reports**              | [GitHub Issue] |
| ğŸ **Feature Requests & Ideas** | [GitHub Issue] |
| ğŸ‘©â€ğŸ’» **Usage Questions**          | [Github Discussions] |
| ğŸ—¯ **General Discussion**       | [Linkedin] or [Gitter Room] |

[GitHub issue]: https://github.com/trungtruc123/ViTTS/issues
[github discussions]: https://github.com/trungtruc123/ViTTS/issues
[gitter room]: https://www.facebook.com/profile.php?id=100038801181933
[linkedin]: https://www.linkedin.com/in/truc-tran-trung-380533149/


## ğŸ”— Links and Resources
| Type                            | Links                               |
| ------------------------------- | --------------------------------------- |
| ğŸ’¼ **Documentation**              | [ReadTheDocs](https://github.com/trungtruc123/ViTTS/tree/develop/docs)
| ğŸ’¾ **Installation**               | [TTS/README.md](https://github.com/trungtruc123/ViTTS/blob/develop/README.md)|
| ğŸ‘©â€ğŸ’» **Contributing**               | [CONTRIBUTING.md](https://github.com/trungtruc123/ViTTS/blob/develop/README.md)|
| ğŸ“Œ **Road Map**                   | [Main Development Plans](https://github.com/trungtruc123/ViTTS/blob/develop/README.md)


## Implemented Models
### Text-to-Spectrogram
- Tacotron: [paper](https://arxiv.org/abs/1703.10135)
- Tacotron2: [paper](https://arxiv.org/abs/1712.05884)
- Glow-TTS: [paper](https://arxiv.org/abs/2005.11129)
- Speedy-Speech: [paper](https://arxiv.org/abs/2008.03802)
- Align-TTS: [paper](https://arxiv.org/abs/2003.01950)
- FastPitch: [paper](https://arxiv.org/pdf/2006.06873.pdf)
- FastSpeech: [paper](https://arxiv.org/abs/1905.09263)

### End-to-End Models
- VITS: [paper](https://arxiv.org/pdf/2106.06103)

### Attention Methods
- Guided Attention: [paper](https://arxiv.org/abs/1710.08969)
- Forward Backward Decoding: [paper](https://arxiv.org/abs/1907.09006)
- Graves Attention: [paper](https://arxiv.org/abs/1910.10288)
- Double Decoder Consistency: [blog](https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/)
- Dynamic Convolutional Attention: [paper](https://arxiv.org/pdf/1910.10288.pdf)
- Alignment Network: [paper](https://arxiv.org/abs/2108.10447)

### Speaker Encoder
- GE2E: [paper](https://arxiv.org/abs/1710.10467)
- Angular Loss: [paper](https://arxiv.org/pdf/2003.11982.pdf)

### Vocoders
- MelGAN: [paper](https://arxiv.org/abs/1910.06711)
- MultiBandMelGAN: [paper](https://arxiv.org/abs/2005.05106)
- ParallelWaveGAN: [paper](https://arxiv.org/abs/1910.11480)
- GAN-TTS discriminators: [paper](https://arxiv.org/abs/1909.11646)
- WaveRNN: [origin](https://github.com/fatchord/WaveRNN/)
- WaveGrad: [paper](https://arxiv.org/abs/2009.00713)
- HiFiGAN: [paper](https://arxiv.org/abs/2010.05646)
- UnivNet: [paper](https://arxiv.org/abs/2106.07889)

You can also help us implement more models.

## Install TTS
